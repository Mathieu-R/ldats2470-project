---
title: "LDATS2470 - Project"
author: "Mathieu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
# clean variables
rm(list = ls())
```

## Loading data

```{r}
library(tidyverse)

# color palettes
library(hrbrthemes)
library(viridis)

# train-test split
library(rsample)

# tables
library(formattable)
library(huxtable)
library(gt)

# skewness,...
library(moments)
library(patchwork)

# correlation
library(corrplot)

# modeling
library(caret)

# SVM
library(kernlab)
library(e1071)

library(pdp)
library(vip)

# PCA
library(MASS)
```

```{r}
source("utils/plots.r")
source("utils/tables.r")
source("utils/misc.r")
```

```{r}
parkinson_data <- read.csv("dataset/parkinsons.csv", sep = ",", dec = ".", header = TRUE)

parkinson_data <- parkinson_data %>%
  rename(
    mdvp.fo = MDVP.Fo.Hz.,
    mdvp.fhi = MDVP.Fhi.Hz.,
    mdvp.flo = MDVP.Flo.Hz.,
    mdvp.jitter_perc = MDVP.Jitter...,
    mdvp.jitter_abs = MDVP.Jitter.Abs.,
    mdvp.rap = MDVP.RAP,
    mdvp.ppq = MDVP.PPQ,
    jitter.ddp = Jitter.DDP,
    mdvp.shimmer = MDVP.Shimmer,
    mdvp.shimmer_db = MDVP.Shimmer.dB.,
    mdvp.apq = MDVP.APQ,
    shimmer.apq3 = Shimmer.APQ3,
    shimmer.apq5 = Shimmer.APQ5,
    shimmer.dda = Shimmer.DDA,
    nhr = NHR,
    hnr = HNR,
    rpde = RPDE,
    dfa = DFA,
    d2 = D2,
    ppe = PPE
  ) %>%
  mutate(status = factor(ifelse(status == "1", "parkinson", "healthy")))

head(parkinson_data)
```

## Descriptive statistics

### Response variable
```{r}
table(parkinson_data$status)

status_percentage <- prop.table(table(parkinson_data$status))
status_plot <- barplot(status_percentage, col = "#3F6D9B", ylab = "Percentage", main = "Health status")
```
### Explanatory variables

```{r out.width = 80%}
parkinson_data %>%
  dplyr::select_if(is.numeric) %>%
  summary_moments %>%
  gt %>%
  tab_header(
    title = "4 moments of quantitative variables"
  )
```

```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.fo), 
  histplot(parkinson_data, mdvp.fhi),
  histplot(parkinson_data, mdvp.flo),
  nrow = 2, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.jitter_perc), 
  histplot(parkinson_data, mdvp.jitter_abs),
  histplot(parkinson_data, mdvp.rap),
  histplot(parkinson_data, mdvp.ppq),
  histplot(parkinson_data, jitter.ddp),
  nrow = 3, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.apq), 
  histplot(parkinson_data, mdvp.shimmer),
  histplot(parkinson_data, mdvp.shimmer_db),
  histplot(parkinson_data, shimmer.apq3),
  histplot(parkinson_data, shimmer.apq5),
  histplot(parkinson_data, shimmer.dda),
  nrow = 3, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, nhr), 
  histplot(parkinson_data, hnr),
  nrow = 1, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, rpde), 
  histplot(parkinson_data, d2),
  nrow = 1, ncol = 2
)
```
```{r}
histplot(parkinson_data, dfa)
```

```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, spread1), 
  histplot(parkinson_data, spread2),
  histplot(parkinson_data, ppe),
  nrow = 2, ncol = 2
)
```
## SVM

```{r}
set.seed(123)

split <- initial_split(parkinson_data, prop = 0.8)
df_train <- training(split)
df_test <- testing(split)

# manage target class imbalance
weights <- ifelse(
  df_train$status == "parkinson", 
  (1 / table(df_train$status)[1]) * 0.5,
  (1 / table(df_train$status)[2]) * 0.5
)

head(df_train)
```

### Linear SVM

Let's split the dataset

```{r}
ctrl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

linear_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmLinear",
  weights = weights,
  preProcess = c("center", "scale"),
  trControl = ctrl
)

confusionMatrix(linear_svm)
```
```{r}
pred <- predict(linear_svm, newdata = df_test)
confusionMatrix(pred, df_test$status)
```

### Kernel trick

```{r}
# 10-fold CV for \gamma et C parameters
ctrl <- trainControl(
  method = "repeatedcv", 
  number = 5, # 5 fold cv
  repeats = 5, # 5 repetitions of cv
  classProbs = TRUE,
  summaryFunction = twoClassSummary # use AUC to pick the best model
)

kernel_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmRadial",
  weights = weights,
  preProcess = c("center", "scale"),
  metric = "ROC",
  trControl = ctrl,
  tuneLength = 10
)

parkinson_svm$results

ggplot(parkinson_svm) +
  theme_minimal()

confusionMatrix(kernel_svm)
```
```{r}
grid <- expand.grid(
  sigma = c(0.0020, 0.0023, 0.0025, 0.0028, 0.0030), 
  C = c(1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5)
)

kernel_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmRadial",
  preProcess = c("center", "scale"),
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = grid
)

kernel_svm$results
```

```{r}
classifier <- svm(status ~ ., data = df_train, type = "C-classification", kernel = "linear", cost = 1, scale = TRUE)
classifier
```
```{r}
plot_svm <- function(x, n) {
  
}
```
```{r}
y_pred <- predict(classifier, newdata = df_test)
```

```{r}
epochs = 10000

C = 12.0
gamma = 0.002

tol = 10e-8
epsilon = 10e-8

custom_svm.rbf_kernel <- function(X_i, X_j, gamma) {
  exp((-norm(as.matrix(X_i - X_j))^2) * gamma)
}

custom_svm.compute_kernel_matrix <- function(X1, X2, gamma) {
  n <- nrow(X1)
  m <- nrow(X2)
  
  K <- matrix(0, nrow = n, ncol = m)
  
  for (i in 1:n) {
    for (j in 1:m) {
      K[i, j] <- custom_svm.rbf_kernel(X1[i,], X2[j,], gamma)
    }
  }
  
  return(K)
}

custom_svm.fit <- function(X, y) {
  n <- nrow(X)
  
  X <- cbind(X, 1)
  y <- ifelse(y == "healthy", -1, 1)
  
  K <- custom_svm.compute_kernel_matrix(X, X, gamma)
  
  etas <- rep(0, times = n)
  
  for (k in 1:n) {
    etas[k] <- 1 / K[k, k]
  }
  
  alphas <- rep(0, times = n)
  
  for (t in 1:epochs) {
    current_alphas <- alphas
    
    # update each alpha component 
    # (stochastic gradient ascent)
    for (k in 1:n) {
      current_alphas[k] <- current_alphas[k] + etas[k] * (1 - (y[k] * sum(current_alphas * y * K[, k])))
      
      # ensure 0 <= alpha_k <= C
      if (current_alphas[k] < 0) {
        current_alphas[k] <- 0
      } else if (current_alphas[k] > C) {
        current_alphas[k] <- C
      }
    }
    
    # break if convergence
    if (norm(as.matrix(current_alphas - alphas)) <= tol) {
      print(paste("converged at iteration:", t))
      break
    }
    
    # update alphas
    alphas <- current_alphas
  }
  
  support_vectors <- custom_svm.compute_support_vectors(X, alphas)
  return(support_vectors)
}

custom_svm.compute_support_vectors <- function(X, alphas) {
  support_vectors_idx <- alphas > epsilon
  support_vectors <- X[support_vectors_idx,][, -1]
  return(support_vectors)
}
```

```{r}
df_train <- df_train %>% dplyr::select(-name)
df_test <- df_test %>% dplyr::select(-name)

X_train <- df_train %>% dplyr::select(-status)
y_train <- df_train %>% dplyr::select(status)

X_test <- df_test %>% dplyr::select(-status)
y_test <- df_test %>% dplyr::select(status)
```

```{r}
X_train_pca <- PCA(X_train, ncp = 2, graph = FALSE)$ind$coord
X_test_pca <- PCA(X_test, ncp = 2, graph = FALSE)$ind$coord

support_vectors <- custom_svm.fit(X_train_pca, y_train)
support_vectors
```

