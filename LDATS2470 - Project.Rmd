---
title: "LDATS2470 - Project"
author: "Mathieu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
# clean variables
rm(list = ls())
```

## Loading data

```{r}
library(tidyverse)

# color palettes
library(hrbrthemes)
library(viridis)

# train-test split
library(rsample)

# tables
library(formattable)
library(huxtable)
library(gt)

# skewness,...
library(moments)
library(patchwork)

# correlation
library(corrplot)

# modeling
library(caret)

# SVM
library(kernlab)
library(e1071)

library(pdp)
library(vip)
```

```{r}
source("utils/plots.r")
source("utils/tables.r")
source("utils/misc.r")
```

```{r}
parkinson_data <- read.csv("dataset/parkinsons.csv", sep = ",", dec = ".", header = TRUE)

parkinson_data <- parkinson_data %>%
  rename(
    mdvp.fo = MDVP.Fo.Hz.,
    mdvp.fhi = MDVP.Fhi.Hz.,
    mdvp.flo = MDVP.Flo.Hz.,
    mdvp.jitter_perc = MDVP.Jitter...,
    mdvp.jitter_abs = MDVP.Jitter.Abs.,
    mdvp.rap = MDVP.RAP,
    mdvp.ppq = MDVP.PPQ,
    jitter.ddp = Jitter.DDP,
    mdvp.shimmer = MDVP.Shimmer,
    mdvp.shimmer_db = MDVP.Shimmer.dB.,
    mdvp.apq = MDVP.APQ,
    shimmer.apq3 = Shimmer.APQ3,
    shimmer.apq5 = Shimmer.APQ5,
    shimmer.dda = Shimmer.DDA,
    nhr = NHR,
    hnr = HNR,
    rpde = RPDE,
    dfa = DFA,
    d2 = D2,
    ppe = PPE
  ) %>%
  mutate(status = factor(ifelse(status == "1", "parkinson", "healthy")))

head(parkinson_data)
```

## Descriptive statistics

### Response variable
```{r}
table(parkinson_data$status)

status_percentage <- prop.table(table(parkinson_data$status))
status_plot <- barplot(status_percentage, col = "#3F6D9B", ylab = "Percentage", main = "Health status")
```
### Explanatory variables

```{r out.width = 80%}
parkinson_data %>%
  dplyr::select_if(is.numeric) %>%
  summary_moments %>%
  gt %>%
  tab_header(
    title = "4 moments of quantitative variables"
  )
```

```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.fo), 
  histplot(parkinson_data, mdvp.fhi),
  histplot(parkinson_data, mdvp.flo),
  nrow = 2, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.jitter_perc), 
  histplot(parkinson_data, mdvp.jitter_abs),
  histplot(parkinson_data, mdvp.rap),
  histplot(parkinson_data, mdvp.ppq),
  histplot(parkinson_data, jitter.ddp),
  nrow = 3, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, mdvp.apq), 
  histplot(parkinson_data, mdvp.shimmer),
  histplot(parkinson_data, mdvp.shimmer_db),
  histplot(parkinson_data, shimmer.apq3),
  histplot(parkinson_data, shimmer.apq5),
  histplot(parkinson_data, shimmer.dda),
  nrow = 3, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, nhr), 
  histplot(parkinson_data, hnr),
  nrow = 1, ncol = 2
)
```
```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, rpde), 
  histplot(parkinson_data, d2),
  nrow = 1, ncol = 2
)
```
```{r}
histplot(parkinson_data, dfa)
```

```{r}
gridExtra::grid.arrange(
  histplot(parkinson_data, spread1), 
  histplot(parkinson_data, spread2),
  histplot(parkinson_data, ppe),
  nrow = 2, ncol = 2
)
```
## SVM

```{r}
set.seed(123)

split <- initial_split(parkinson_data, prop = 0.8)
df_train <- training(split)
df_test <- testing(split)

# manage target class imbalance
weights <- ifelse(
  df_train$status == "parkinson", 
  (1 / table(df_train$status)[1]) * 0.5,
  (1 / table(df_train$status)[2]) * 0.5
)

head(df_train)
```

### Linear SVM

Let's split the dataset

```{r}
ctrl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

linear_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmLinear",
  weights = weights,
  preProcess = c("center", "scale"),
  trControl = ctrl
)

confusionMatrix(linear_svm)
```
```{r}
pred <- predict(linear_svm, newdata = df_test)
confusionMatrix(pred, df_test$status)
```

### Kernel trick

```{r}
# 10-fold CV for \gamma et C parameters
ctrl <- trainControl(
  method = "repeatedcv", 
  number = 5, # 5 fold cv
  repeats = 5, # 5 repetitions of cv
  classProbs = TRUE,
  summaryFunction = twoClassSummary # use AUC to pick the best model
)

kernel_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmRadial",
  weights = weights,
  preProcess = c("center", "scale"),
  metric = "ROC",
  trControl = ctrl,
  tuneLength = 10
)

parkinson_svm$results

ggplot(parkinson_svm) +
  theme_minimal()

confusionMatrix(kernel_svm)
```
```{r}
grid <- expand.grid(
  sigma = c(0.0020, 0.0023, 0.0025, 0.0028, 0.0030), 
  C = c(1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5)
)

kernel_svm <- train(
  status ~ .,
  data = df_train,
  method = "svmRadial",
  preProcess = c("center", "scale"),
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = grid
)

kernel_svm$results
```

```{r}
classifier <- svm(status ~ ., data = df_train, type = "C-classification", kernel = "linear", cost = 1, scale = TRUE)
classifier
```
```{r}
plot_svm <- function(x, n) {
  
}
```
```{r}
y_pred <- predict(classifier, newdata = df_test)
```

```{r}
rm(list = ls(all.names = TRUE))
library(e1071)
library(caret)
library(ggplot2)
library(ElemStatLearn)
#install.packages('C:/Users/rosta/Downloads/ElemStatLearn_2015.6.26.tar', repos=NULL, type='source')

data1=read.csv("E:/UCL Courses/LDATS2470- Statistical Machine Learning and High Dimensional Data Analysis/Project/parkinsons.csv")
sum(is.na(data1))
data2=data1[,-c(1,18)]
data3=data.frame(data2,data1[,18])
colnames(data3)[23]  <- "status"
data=data3
dim(data)
head(data)
data$status <- factor(data$status, levels = c(0, 1))
names(data)

#drawing plot for two variables 
plot(data[, -3],
     main = 'Scatter Plot', col=c("red", "blue"),
     xlab = 'data[,5]', ylab = 'data[,6]',
     xlim = range(data[,1]), ylim = range(data[,2]))

set.seed(123)
trainIndex <- createDataPartition(data$status, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
dim(trainData)
dim(testData)
trainData[,-23]=scale(trainData[,-23])
testData[-23]=scale(testData[,-23])

# Define a range of regularization parameter values to try
cost_range <- 10^(-3:3)

# Perform cross-validation to choose the best regularization parameter
set.seed(123)
cv_results <- tune.svm(status ~ ., data = trainData, kernel = "radial", cost = cost_range, gamma=c(0.1, 1, 10), cross=10)
cv_results

# Get best parameters from cross-validation
best_cost <- cv_results$best.parameters$cost
best.gamma <- cv_results$best.parameters$gamma


# Train SVM model with best parameters
classifier <- svm(status ~ ., data = trainData, type = 'C-classification', kernel = "radial", cost = best_cost, gamma=best.gamma, scale=FALSE)
#classifier <- svm(status ~ ., data = trainData, type = 'C-classification', kernel = "linear", scale=FALSE)

# Make predictions on the test set
testPred <- predict(classifier, newdata= testData[,-23])
head(testPred)
length(testPred)

# Calculate the objective function (for soft margin case)
obj_fun <- sum(classifier$omega^2) / 2 + sum(classifier$xi[classifier$y == 1]) + sum(classifier$xi[classifier$y == -1])
obj_fun

#confusion matrix
cm = table(testData[,23], testPred)
cm

# Calculate the proportion of correct classifications (method 1 for calculating accuracy)
accuracy <- mean(testPred == testData$status)
print(paste0("Accuracy: ", round(accuracy, 2)))

# To show conducting linear nonseparable SVM with RBF kernel with just two variables
set = trainData[,c(1,2,23)]
X1 = seq(min(set[, 1]) - 2, max(set[, 1]) + 3, by = 0.01)
X2 = seq(min(set[, 2]) - 2, max(set[, 2]) + 4, by = 0.01)
length(X1)
length(X2)
grid_set = expand.grid(X1, X2)
dim(grid_set)
head(grid_set)
names(trainData)
colnames(grid_set) = c('MDVP.Fo.Hz.', 'MDVP.Fhi.Hz.')
classifier_grid <- svm(status ~ ., data = trainData[,c(1,2,23)], type = 'C-classification', kernel = "radial", scale=FALSE)
y_grid = predict(classifier_grid, newdata = grid_set)
length(y_grid)
plot(set[, -3],
     main = 'Kernel SVM (Training set)',
     xlab = 'MDVP.Fo.Hz.', ylab = 'MDVP.Fhi.Hz.',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

dev.off() # To remove the previous graphs

################################### drawing the graph of SVM with linear kernel and with two variables #############################

rm(list = ls(all.names = TRUE))
library(e1071)
library(caret)
library(ggplot2)
library(ElemStatLearn)

data1=read.csv("E:/UCL Courses/LDATS2470- Statistical Machine Learning and High Dimensional Data Analysis/Project/parkinsons.csv")
sum(is.na(data1))
data_2=data1[,-c(1,18)]
data2=scale(data_2)
x=matrix(c(data2[,2], data2[,3]), ncol=2)
y=data1$status
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
data=data.frame(x, y = as.factor(y))

set.seed(123)
trainIndex <- createDataPartition(data$y, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
dim(trainData)
dim(testData)
trainData[,-3]=scale(trainData[,-3])
testData[-3]=scale(testData[,-3])

svmfit = svm(y ~ ., data = trainData, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)

plot(svmfit, data)

make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}

xgrid = make.grid(x)

ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)

beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho

plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19) # indicating the points of two variables
#points(x[svmfit$index,], pch = 5, cex = 2) # indicating support vectors
abline(beta0 / beta[2], -beta[1] / beta[2]) # hyperplane
abline((beta0 -1) / beta[2]- 0.5, -beta[1] / beta[2], lty = 2) # lower margin (in order for the graph to be displayed clearly and not overlapped, we add (-30) ) to equation)
abline((beta0 + 1) / beta[2]+ 0.5, -beta[1] / beta[2], lty = 2) # upper margin (in order for the graph to be displayed clearly and not overlapped, we add (-30) ) to equation)

# Make predictions on the test set
testPred <- predict(classifier, newdata= testData[,-3])
head(testPred)
length(testPred)

dev.off() # To remove the previous graphs
```
